{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-term Exam\n",
    "## Frances Zi Yang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  Import the spam dataset and print the first six rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change\n",
    "# I read the file from git hub url instead of your local place\n",
    "# This will improve two import qualities: portability and scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "url = \"https://raw.githubusercontent.com/QMSS-GR5069-Spring-2022/gr5069-homework-2-franyang6/main/spam_dataset.csv\"\n",
    "\n",
    "df1 = pd.read_csv(url, on_bad_lines='skip')\n",
    "\n",
    "\n",
    "\n",
    "# df1 = pd.read_csv(\"/Users/FrancesY/Documents/Columbia/spring2022/5073ml/midterm/spam_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make:</th>\n",
       "      <th>word_freq_address:</th>\n",
       "      <th>word_freq_all:</th>\n",
       "      <th>word_freq_3d:</th>\n",
       "      <th>word_freq_our:</th>\n",
       "      <th>word_freq_over:</th>\n",
       "      <th>word_freq_remove:</th>\n",
       "      <th>word_freq_internet:</th>\n",
       "      <th>word_freq_order:</th>\n",
       "      <th>word_freq_mail:</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;:</th>\n",
       "      <th>char_freq_(:</th>\n",
       "      <th>char_freq_[:</th>\n",
       "      <th>char_freq_!:</th>\n",
       "      <th>char_freq_$:</th>\n",
       "      <th>char_freq_#:</th>\n",
       "      <th>capital_run_length_average:</th>\n",
       "      <th>capital_run_length_longest:</th>\n",
       "      <th>capital_run_length_total:</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make:  word_freq_address:  word_freq_all:  word_freq_3d:  \\\n",
       "0             0.00                0.64            0.64            0.0   \n",
       "1             0.21                0.28            0.50            0.0   \n",
       "2             0.06                0.00            0.71            0.0   \n",
       "3             0.00                0.00            0.00            0.0   \n",
       "4             0.00                0.00            0.00            0.0   \n",
       "5             0.00                0.00            0.00            0.0   \n",
       "\n",
       "   word_freq_our:  word_freq_over:  word_freq_remove:  word_freq_internet:  \\\n",
       "0            0.32             0.00               0.00                 0.00   \n",
       "1            0.14             0.28               0.21                 0.07   \n",
       "2            1.23             0.19               0.19                 0.12   \n",
       "3            0.63             0.00               0.31                 0.63   \n",
       "4            0.63             0.00               0.31                 0.63   \n",
       "5            1.85             0.00               0.00                 1.85   \n",
       "\n",
       "   word_freq_order:  word_freq_mail:  ...  char_freq_;:  char_freq_(:  \\\n",
       "0              0.00             0.00  ...          0.00         0.000   \n",
       "1              0.00             0.94  ...          0.00         0.132   \n",
       "2              0.64             0.25  ...          0.01         0.143   \n",
       "3              0.31             0.63  ...          0.00         0.137   \n",
       "4              0.31             0.63  ...          0.00         0.135   \n",
       "5              0.00             0.00  ...          0.00         0.223   \n",
       "\n",
       "   char_freq_[:  char_freq_!:  char_freq_$:  char_freq_#:  \\\n",
       "0           0.0         0.778         0.000         0.000   \n",
       "1           0.0         0.372         0.180         0.048   \n",
       "2           0.0         0.276         0.184         0.010   \n",
       "3           0.0         0.137         0.000         0.000   \n",
       "4           0.0         0.135         0.000         0.000   \n",
       "5           0.0         0.000         0.000         0.000   \n",
       "\n",
       "   capital_run_length_average:  capital_run_length_longest:  \\\n",
       "0                        3.756                           61   \n",
       "1                        5.114                          101   \n",
       "2                        9.821                          485   \n",
       "3                        3.537                           40   \n",
       "4                        3.537                           40   \n",
       "5                        3.000                           15   \n",
       "\n",
       "   capital_run_length_total:  spam  \n",
       "0                        278     1  \n",
       "1                       1028     1  \n",
       "2                       2259     1  \n",
       "3                        191     1  \n",
       "4                        191     1  \n",
       "5                         54     1  \n",
       "\n",
       "[6 rows x 58 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first six rows\n",
    "df1.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  The dependent variable is \"spam\" where one indicates that an email is spam and zero otherwise.  Which three variables in the dataset do you think will be important predictors in a model of spam?  Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three variables in the dataset that might be important predictors in the model of spam: \n",
    "\n",
    "1. \"capital_run_length_total:\", which is the total number of capital letters in the e-mail, can be an important predictor because when I personally got spam emails many of the letters are capitalized randomly; \n",
    "\n",
    "2. \"char_freq_!:\" can also be an important predictor since people rarely use exclamation mark in a professional setting, close relatives or friends mostly text not email;\n",
    "\n",
    "3. \"word_freq_address:\", since the most spam emails are trying to get your personal info and address is one of the most important piece of your personal info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Visualize the univariate distribution of each of the variables in the previous question.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# \"spam\"\n",
    "plt.hist(df1['spam'])\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAQ/UlEQVR4nO3dfYxc1XnH8e8TO5C3Ftt4SV3b6prGqWL+CFCXmKatUkjBQISpFCSjqHETV1YbGpG+paZIpXlBgqQqCLV5QcGtSSnGJbQgkgi5BPryR0yWdww4XsAJG1C8yECaRkFx8vSPe4zHy+7OzHp2ZvH5fqTRnHvumbnPPd7Z3869d8aRmUiS6vO6QRcgSRoMA0CSKmUASFKlDABJqpQBIEmVmj/oAqazePHiHB4eHnQZkvSact999z2fmUPtxs3pABgeHmZkZGTQZUjSa0pEfKeTcR4CkqRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaAJFXKAJCkSs3pTwIfqeHNXx3Idvdeed5AtitJ3fAdgCRVygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaAJFWq4wCIiHkR8UBE3FGWV0TEzojYExE3R8Qxpf/Ysjxa1g+3PMelpX93RJzd652RJHWum3cAlwCPtyxfBVydmSuBF4CNpX8j8EJmvg24uowjIlYB64GTgLXA5yJi3pGVL0maqY4CICKWAecBXyrLAZwB3FKGbAUuKO11ZZmy/swyfh2wLTNfzsyngVHgtF7shCSpe52+A7gG+Djws7J8PPBiZh4oy2PA0tJeCjwDUNa/VMa/0j/JYyRJfdY2ACLifcC+zLyvtXuSodlm3XSPad3epogYiYiR8fHxduVJkmaok3cA7wbOj4i9wDaaQz/XAAsiYn4Zswx4trTHgOUAZf1xwP7W/kke84rMvC4zV2fm6qGhoa53SJLUmbYBkJmXZuayzBymOYn7jcz8AHA38P4ybANwW2nfXpYp67+RmVn615erhFYAK4F7e7YnkqSuzG8/ZEp/CWyLiE8DDwDXl/7rgS9HxCjNX/7rATJzV0RsBx4DDgAXZ+ZPj2D7kqQj0FUAZOY9wD2l/RSTXMWTmT8GLpzi8VcAV3RbpCSp9/wksCRVygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaAJFXKAJCkShkAklQpA0CSKmUASFKlDABJqpQBIEmVMgAkqVIGgCRVygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqVNsAiIg3RMS9EfFQROyKiE+U/hURsTMi9kTEzRFxTOk/tiyPlvXDLc91aenfHRFnz9ZOSZLa6+QdwMvAGZn5TuBkYG1ErAGuAq7OzJXAC8DGMn4j8EJmvg24uowjIlYB64GTgLXA5yJiXi93RpLUubYBkI0flsXXl1sCZwC3lP6twAWlva4sU9afGRFR+rdl5suZ+TQwCpzWk72QJHWto3MAETEvIh4E9gE7gCeBFzPzQBkyBiwt7aXAMwBl/UvA8a39kzymdVubImIkIkbGx8e73yNJUkc6CoDM/Glmngwso/mr/R2TDSv3McW6qfonbuu6zFydmauHhoY6KU+SNANdXQWUmS8C9wBrgAURMb+sWgY8W9pjwHKAsv44YH9r/ySPkST1WSdXAQ1FxILSfiPwXuBx4G7g/WXYBuC20r69LFPWfyMzs/SvL1cJrQBWAvf2akckSd2Z334IS4Ct5Yqd1wHbM/OOiHgM2BYRnwYeAK4v468HvhwRozR/+a8HyMxdEbEdeAw4AFycmT/t7e5IkjrVNgAy82HglEn6n2KSq3gy88fAhVM81xXAFd2XKUnqNT8JLEmVMgAkqVIGgCRVygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaAJFXKAJCkShkAklQpA0CSKmUASFKlDABJqpQBIEmVMgAkqVIGgCRVygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVKm2ARARyyPi7oh4PCJ2RcQlpX9RROyIiD3lfmHpj4i4NiJGI+LhiDi15bk2lPF7ImLD7O2WJKmdTt4BHAD+LDPfAawBLo6IVcBm4K7MXAncVZYBzgFWltsm4PPQBAZwOfAu4DTg8oOhIUnqv7YBkJnPZeb9pf2/wOPAUmAdsLUM2wpcUNrrgBuy8U1gQUQsAc4GdmTm/sx8AdgBrO3p3kiSOtbVOYCIGAZOAXYCb83M56AJCeCEMmwp8EzLw8ZK31T9E7exKSJGImJkfHy8m/IkSV3oOAAi4i3AV4CPZeYPphs6SV9O0394R+Z1mbk6M1cPDQ11Wp4kqUsdBUBEvJ7ml/+NmXlr6f5+ObRDud9X+seA5S0PXwY8O02/JGkAOrkKKIDrgccz8+9aVt0OHLySZwNwW0v/B8vVQGuAl8ohojuBsyJiYTn5e1bpkyQNwPwOxrwb+D3gkYh4sPT9FXAlsD0iNgLfBS4s674GnAuMAj8CPgSQmfsj4lPAt8q4T2bm/p7shSSpa20DIDP/h8mP3wOcOcn4BC6e4rm2AFu6KVCSNDv8JLAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaAJFXKAJCkShkAklQpA0CSKmUASFKlDABJqpQBIEmVMgAkqVIGgCRVygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaAJFXKAJCkShkAklQpA0CSKmUASFKl2gZARGyJiH0R8WhL36KI2BERe8r9wtIfEXFtRIxGxMMRcWrLYzaU8XsiYsPs7I4kqVOdvAP4J2DthL7NwF2ZuRK4qywDnAOsLLdNwOehCQzgcuBdwGnA5QdDQ5I0GG0DIDP/C9g/oXsdsLW0twIXtPTfkI1vAgsiYglwNrAjM/dn5gvADl4dKpKkPprpOYC3ZuZzAOX+hNK/FHimZdxY6ZuqX5I0IL0+CRyT9OU0/a9+gohNETESESPj4+M9LU6SdMhMA+D75dAO5X5f6R8DlreMWwY8O03/q2TmdZm5OjNXDw0NzbA8SVI7Mw2A24GDV/JsAG5r6f9guRpoDfBSOUR0J3BWRCwsJ3/PKn2SpAGZ325ARNwEvAdYHBFjNFfzXAlsj4iNwHeBC8vwrwHnAqPAj4APAWTm/oj4FPCtMu6TmTnxxLIkqY/aBkBmXjTFqjMnGZvAxVM8zxZgS1fVSZJmjZ8ElqRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaAJFXKAJCkShkAklQpA0CSKmUASFKlDABJqpQBIEmVMgAkqVIGgCRVygCQpEoZAJJUKQNAkiplAEhSpQwASaqUASBJlTIAJKlSBoAkVcoAkKRKGQCSVCkDQJIqZQBIUqUMAEmqlAEgSZUyACSpUgaAJFXKAJCkShkAklQpA0CSKmUASFKl5g+6gKPR8OavDmS7e688byDblfTa5DsASapU3wMgItZGxO6IGI2Izf3eviSp0dcAiIh5wD8A5wCrgIsiYlU/a5AkNfp9DuA0YDQznwKIiG3AOuCxPtdxVBrUuQfw/IP0WtTvAFgKPNOyPAa8q3VARGwCNpXFH0bE7iPY3mLg+SN4/Gw56uqKq3pcyasddXPWB3O1trlaF8zd2rqt65c6GdTvAIhJ+vKwhczrgOt6srGIkcxc3Yvn6iXr6t5crW2u1gVzt7a5WhfM3dpmq65+nwQeA5a3LC8Dnu1zDZIk+h8A3wJWRsSKiDgGWA/c3ucaJEn0+RBQZh6IiD8G7gTmAVsyc9csbrInh5JmgXV1b67WNlfrgrlb21ytC+ZubbNSV2Rm+1GSpKOOnwSWpEoZAJJUqaMyAPr9dRMRsTwi7o6IxyNiV0RcUvoXRcSOiNhT7heW/oiIa0t9D0fEqS3PtaGM3xMRG3pU37yIeCAi7ijLKyJiZ9nGzeWEPBFxbFkeLeuHW57j0tK/OyLO7lFdCyLiloh4oszd6XNhziLiT8q/46MRcVNEvGFQcxYRWyJiX0Q82tLXszmKiF+NiEfKY66NiMku1e60rs+Wf8uHI+LfImJBu7mY6rU61XzPtLaWdX8eERkRi+fCnJX+j5Y52BURn+nrnGXmUXWjObn8JHAicAzwELBqlre5BDi1tH8O+DbNV118Bthc+jcDV5X2ucDXaT4XsQbYWfoXAU+V+4WlvbAH9f0p8C/AHWV5O7C+tL8A/FFpfwT4QmmvB24u7VVlHo8FVpT5ndeDurYCf1DaxwALBj1nNB9WfBp4Y8tc/f6g5gz4LeBU4NGWvp7NEXAvcHp5zNeBc46grrOA+aV9VUtdk84F07xWp5rvmdZW+pfTXIDyHWDxHJmz3wb+Azi2LJ/QzzmbtV+Kg7qVf5g7W5YvBS7tcw23Ab8D7AaWlL4lwO7S/iJwUcv43WX9RcAXW/oPGzfDWpYBdwFnAHeUH9rnW16or8xXeXGcXtrzy7iYOIet446grp+n+UUbE/oHOmcc+rT6ojIHdwBnD3LOgOEJvzR6Mkdl3RMt/YeN67auCet+F7ixtCedC6Z4rU73M3oktQG3AO8E9nIoAAY6ZzS/tN87ybi+zNnReAhosq+bWNqvjZdDAKcAO4G3ZuZzAOX+hDY1zkbt1wAfB35Wlo8HXszMA5Ns45Xtl/UvlfGzUdeJwDjwj9EcnvpSRLyZAc9ZZn4P+Fvgu8BzNHNwH3Njzg7q1RwtLe3ZqPHDNH8dz6Su6X5GZyQizge+l5kPTVg16Dl7O/Cb5dDNf0bEr82wrhnN2dEYAG2/bmLWNhzxFuArwMcy8wfTDZ2kL6fpn2k97wP2ZeZ9HWy7b3UV82neDn8+M08B/o/mcMZU+jVnC2m+oHAF8IvAm2m+vXaqbfRzztrptpZZqTEiLgMOADfOhboi4k3AZcBfT7Z6kLXRvA4W0hx++gtgezmn0Je6jsYAGMjXTUTE62l++d+YmbeW7u9HxJKyfgmwr02Nva793cD5EbEX2EZzGOgaYEFEHPwQYOs2Xtl+WX8csH8W6jq4rbHM3FmWb6EJhEHP2XuBpzNzPDN/AtwK/DpzY84O6tUcjZV2z2osJ0vfB3wgy7GIGdT1PFPP90z8Mk2gP1ReC8uA+yPiF2ZQW6/nbAy4NRv30rxTXzyDumY2ZzM5JjmXbzSJ+hTNP/jBkyQnzfI2A7gBuGZC/2c5/GTdZ0r7PA4/8XRv6V9Ec1x8Ybk9DSzqUY3v4dBJ4H/l8JNFHyntizn8hOb20j6Jw09IPUVvTgL/N/Arpf03Zb4GOmc03067C3hT2dZW4KODnDNefdy4Z3NE8/Usazh0QvPcI6hrLc1Xuw9NGDfpXDDNa3Wq+Z5pbRPW7eXQOYBBz9kfAp8s7bfTHN6Jfs3ZrP1SHOSN5sz+t2nOll/Wh+39Bs3brYeBB8vtXJrjcncBe8r9wR+goPmPcZ4EHgFWtzzXh4HRcvtQD2t8D4cC4ESaKxlGyw/NwSsQ3lCWR8v6E1sef1mpdzcdXvXQQU0nAyNl3v69vNAGPmfAJ4AngEeBL5cX4UDmDLiJ5lzET2j++tvYyzkCVpf9fBL4eyaclO+yrlGaX2AHXwNfaDcXTPFanWq+Z1rbhPV7ORQAg56zY4B/Ls93P3BGP+fMr4KQpEodjecAJEkdMAAkqVIGgCRVygCQpEoZAJJUKQNAkiplAEhSpf4fHkqPa1irePcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# \"capital_run_length_total:\"\n",
    "plt.hist(df1['capital_run_length_total:'])\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# \"char_freq_!:\"\n",
    "plt.hist(df1['char_freq_!:'])\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# \"word_freq_address:\"\n",
    "plt.hist(df1['word_freq_address:'])\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Name each of the supervised learning models that we have learned thus far that are used to predict dependent variables like \"spam\".   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. K-nearest neighbors (KNN) classifier\n",
    "2. Logistic Regression none-penalized\n",
    "3. Logistic Regression with L1-penalty\n",
    "4. Logistic Regression with L2-penalty\n",
    "5. Support Vector classifier\n",
    "6. Decision tree classifier\n",
    "7. Ensemble methods (Bagging classifier, Boosting classifier)\n",
    "8. Random forest classifier (another ensemble method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Describe the importance of training and test data.  Why do we separate data into these subsets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By separating data into training & testing subsets, we can reduce/minimize the effects of data inconsistencies and have a better understanding of the model's characteristics by utilizing similar and randomly assigned data for training and testing: since the testing set data already has known real values for the features we want to predict, it is easier to evaluate the model's accuracy.\n",
    "\n",
    "On top of that, computational efficiency is also a reason for separating data into subsets, some models (for example deep neural network even though we still haven't discussed it in class yet) are expensive to train and repetitive evaluation that are used in other procedures is not manageable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. What is k-fold cross validation and what do we use it for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For k-fold cross validation, we randomly shuffle and split the data into k folds without replacement (for example if we choose k=10 then it's a 10-fold cross validation), with k-1 folds being used for training and 1 fold being used for testing, we repeat this process k times so that we obtain k number of accuracy evaluations for each iteration, after which we get the mean of k number of accuracy evaluations.\n",
    "\n",
    "K-fold cross validation allow us to utilize our data sample and estimate how models are expected to perform (for the testing data) and used to develop a more generalized model (better at predicting unseen data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. How is k-fold cross validation different from stratified k-fold cross validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I explained the process of k-fold cross validation in question 6, the difference between k-fold and stratified k-fold is that k-fold splits the data into k folds, but each fold in a stratified k-fold has the same percentage (ratio) of each class as the original data.\n",
    "\n",
    "Stratified k-fold cross validation is my preferred way compared to k-fold cross validation, especially when we are classifying and the origin data's class ratio is unbalanced (for example 70%/30%, 80%/20%), as plain k-fold cross validation does not take unbalanced class ratio into consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Choose one model from question four.  Split the data into training and test subsets.  Build a model with the three variables in the dataset that you think will be good predictors of \"spam\".  Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k).  Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm using KNN first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: spam, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>capital_run_length_total:</th>\n",
       "      <th>char_freq_!:</th>\n",
       "      <th>word_freq_address:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>278</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1028</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2259</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>191</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>191</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   capital_run_length_total:  char_freq_!:  word_freq_address:\n",
       "0                        278         0.778                0.64\n",
       "1                       1028         0.372                0.28\n",
       "2                       2259         0.276                0.00\n",
       "3                        191         0.137                0.00\n",
       "4                        191         0.135                0.00"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df1['spam']\n",
    "X = df1[['capital_run_length_total:', 'char_freq_!:', 'word_freq_address:']]\n",
    "\n",
    "print(y[0:5])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4601, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3450, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train test split, setting random_state = 42\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "print(X.shape)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using standard scaler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.867\n",
      "Test set score: 0.827\n",
      "[0 0 0 ... 1 0 0]\n",
      "Mean Cross Validation, KFold: 0.821\n"
     ]
    }
   ],
   "source": [
    "# Using scaled data with default parameter\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "kfold = KFold()\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier().fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(knn.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(knn.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Prediction error on test data\n",
    "y_pred_vals = knn.predict(X_test_scaled)\n",
    "print(y_pred_vals)\n",
    "\n",
    "# Kfold Cross Validation\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(knn, X_train_scaled, y_train, cv = kfold))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameter: {'n_neighbors': 6}\n",
      "Test set Score: 0.820\n",
      "[0 0 0 ... 0 0 0]\n",
      "Best Cross-Validation Score: 0.832\n"
     ]
    }
   ],
   "source": [
    "# Tune the parameters of the model using GridSearchCV and scaled data\n",
    "\n",
    "knn_param_grid = {'n_neighbors': range(1, 10)}\n",
    "knn_grid = GridSearchCV(KNeighborsClassifier(), knn_param_grid, cv = kfold).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best Parameter: {}\".format(knn_grid.best_params_))\n",
    "print(\"Test set Score: {:.3f}\".format(knn_grid.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Prediction error on test data\n",
    "y_pred_vals = knn_grid.predict(X_test_scaled)\n",
    "print(y_pred_vals)\n",
    "\n",
    "# Kfold Cross Validation\n",
    "print(\"Best Cross-Validation Score: {:.3f}\".format(knn_grid.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used KNN with default parameter (k equals to 5) with standard scaled data, the test set score is 0.827 and the k-fold cross validation score is 0.821.\n",
    "\n",
    "Then I used GridSearchCV with scaled data to tune the n_neighbors parameter, the best n_neighbor parameter is 6 in this case, test set score is 0.820 and the k-fold cross validation score is 0.832.\n",
    "\n",
    "Tuning in the best parameter did improve with the performance of the model, k-fold cross validation-wise. The method I chose a specific value for k is using GridSearchCV to tune the n_neighbors parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Choose a second model from question four.  Using the same three variables in the dataset that you think will be good predictors of \"spam\".  Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k).  Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  Did this model predict test data better than your previous model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm using logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.750\n",
      "Test set score: 0.749\n",
      "[0 0 0 ... 1 0 0]\n",
      "Mean Cross Validation, KFold: 0.752\n"
     ]
    }
   ],
   "source": [
    "# Using scaled data with default parameter with penalty = 'none'\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(penalty = 'none').fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Prediction error on test data\n",
    "l_pred_vals = logreg.predict(X_test_scaled)\n",
    "print(l_pred_vals)\n",
    "\n",
    "# Kfold Cross Validation\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(logreg, X_train_scaled, y_train, cv = kfold))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.750\n",
      "Test set score: 0.749\n",
      "[0 0 0 ... 1 0 0]\n",
      "Mean Cross Validation, KFold: 0.751\n"
     ]
    }
   ],
   "source": [
    "# Using scaled data with default parameter with penalty = 'l1'\n",
    "logreg = LogisticRegression(penalty = 'l1',solver='liblinear').fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Prediction error on test data\n",
    "l_pred_vals = logreg.predict(X_test_scaled)\n",
    "print(l_pred_vals)\n",
    "\n",
    "# Kfold Cross Validation\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(logreg, X_train_scaled, y_train, cv = kfold))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.750\n",
      "Test set score: 0.749\n",
      "[0 0 0 ... 1 0 0]\n",
      "Mean Cross Validation, KFold: 0.751\n"
     ]
    }
   ],
   "source": [
    "# Using scaled data with default parameter with penalty = 'l2'\n",
    "logreg = LogisticRegression(penalty = 'l2').fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Prediction error on test data\n",
    "l_pred_vals = logreg.predict(X_test_scaled)\n",
    "print(l_pred_vals)\n",
    "\n",
    "# Kfold Cross Validation\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(logreg, X_train_scaled, y_train, cv = kfold))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameter: {'C': 0.001, 'penalty': 'none'}\n",
      "Test set Score: 0.749\n",
      "[0 0 0 ... 1 0 0]\n",
      "Best Cross-Validation Score: 0.752\n"
     ]
    }
   ],
   "source": [
    "# Tune the parameters of the model using GridSearchCV and scaled data\n",
    "logreg_param_grid = {'penalty': ['none','l1','l2'],\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "logreg_grid = GridSearchCV(LogisticRegression(), logreg_param_grid, cv = kfold).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best Parameter: {}\".format(logreg_grid.best_params_))\n",
    "print(\"Test set Score: {:.3f}\".format(logreg_grid.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Prediction error on test data\n",
    "l_pred_vals = logreg_grid.predict(X_test_scaled)\n",
    "print(l_pred_vals)\n",
    "\n",
    "# Kfold Cross Validation\n",
    "print(\"Best Cross-Validation Score: {:.3f}\".format(logreg_grid.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used logistic regression with default parameter and penalty = 'none' with scaled data first, the test set score for that is 0.749 and the k-fold cross validation score is 0.752.\n",
    "\n",
    "Default parameter and penalty = 'l1' with scaled data results in a test set score of 0.749 and k-fold cross validation score of 0.751.\n",
    "\n",
    "Default parameter and penalty = 'l2' with scaled data results in a test set score of 0.749 and k-fold cross validation score of 0.751.\n",
    "\n",
    "I then used GridSearchCV with unscaled data to tune the 'C' and 'penalty' parameter, the best 'C' parameter for that is 0.001 and 'penalty' is none. The test set score in this situation is 0.749 and the k-fold cross validation score is 0.752.\n",
    "\n",
    "This model did not predict test data better than my previous KNN model with standard scaled data. Logistic regression's final test set score with tuned parameter is 0.749 compared to KNN's 0.820 and logistic's final best cross validation score with k-fold and tuned parameter is 0.752 compared to KNN's 0.832."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Choose a third model from question four.  Using the same three variables in the dataset that you think will be good predictors of \"spam\".  Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k). Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  Did this model predict test data better than your previous models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using decision trees for this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.957\n",
      "Test set score: 0.820\n",
      "[0 0 0 ... 1 0 0]\n",
      "Mean Cross Validation, KFold: 0.823\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "tree = decision_tree.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(tree.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(tree.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Prediction error on test data\n",
    "tree_pred_vals = tree.predict(X_test_scaled)\n",
    "print(tree_pred_vals)\n",
    "\n",
    "# Kfold Cross Validation\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(tree, X_train_scaled, y_train, cv = kfold))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameter: {'criterion': 'gini', 'max_depth': 8, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "Test set Score: 0.828\n",
      "[0 0 0 ... 1 0 0]\n",
      "Best Cross-Validation Score: 0.839\n"
     ]
    }
   ],
   "source": [
    "# Using GridSearchCV\n",
    "param_dict = {\n",
    "    \"criterion\":['gini','entropy'],\n",
    "    \"max_depth\":range(1,10),\n",
    "    \"min_samples_split\":range(1,10),\n",
    "    \"min_samples_leaf\":range(1,5)\n",
    "}\n",
    "\n",
    "tree_grid = GridSearchCV(DecisionTreeClassifier(random_state=42), param_dict, cv = kfold).fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "print(\"Best Parameter: {}\".format(tree_grid.best_params_))\n",
    "print(\"Test set Score: {:.3f}\".format(tree_grid.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Prediction error on test data\n",
    "tree_pred_vals = tree_grid.predict(X_test_scaled)\n",
    "print(tree_pred_vals)\n",
    "\n",
    "# Kfold Cross Validation\n",
    "print(\"Best Cross-Validation Score: {:.3f}\".format(tree_grid.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used decision tree classifier with default parameter with scaled data first, the test set score for that is 0.820 and the k-fold cross validation score is 0.823.\n",
    "\n",
    "I then used GridSearchCV with unscaled data to tune the 'criterion', 'max_depth', 'min_samples_split' and 'min_samples_leaf' parameter, the best 'criterion' parameter for that is 'gini', 'max_depth' is 8, 'min_samples_leaf' is 4 and 'min_samples_split' is 2. The test set score in this situation is 0.828 and the k-fold cross validation score is 0.839.\n",
    "\n",
    "This model did predict test data better than my previous KNN and logistic model with standard scaled data. Decision tree's final test set score with tuned parameter is 0.828 comparing to Logistic regression's 0.749 and KNN's 0.820; The best cross-validation score with kfold and tuned parameter using decision tree is 0.839 compared to logistic's 0.752 and KNN's 0.832."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Choose a fourth model from question four.  Using the same three variables in the dataset that you think will be good predictors of \"spam\".  Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k). Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  Did this model predict test data better than your previous models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.957\n",
      "Test set score: 0.836\n",
      "[0 0 0 ... 1 0 0]\n",
      "Mean Cross Validation, KFold: 0.831\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RandomForestClassifier = RandomForestClassifier(random_state=42)\n",
    "rfc = RandomForestClassifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(rfc.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(rfc.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Prediction error on test data\n",
    "rfc_pred_vals = rfc.predict(X_test_scaled)\n",
    "print(rfc_pred_vals)\n",
    "\n",
    "# Kfold Cross Validation\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(rfc, X_train_scaled, y_train, cv = kfold))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best Parameter: {'oob_score': True, 'n_estimators': 1000, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 9, 'criterion': 'gini'}\n",
      "Test set Score: 0.844\n",
      "[0 0 0 ... 1 0 0]\n",
      "Best Cross-Validation Score: 0.846\n",
      "{'bootstrap': True,\n",
      " 'ccp_alpha': 0.0,\n",
      " 'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 9,\n",
      " 'max_features': 'log2',\n",
      " 'max_leaf_nodes': None,\n",
      " 'max_samples': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 1000,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': True,\n",
      " 'random_state': 42,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# Using RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_dict = {\n",
    "    \"criterion\":['gini','entropy'],\n",
    "    \"max_depth\":range(1,10),\n",
    "    \"n_estimators\":[200, 500, 1000, 2000],\n",
    "    \"min_samples_leaf\":range(1,5),\n",
    "    \"max_features\": ['auto', 'sqrt', 'log2'],\n",
    "    \"oob_score\": [True, False]\n",
    "}\n",
    "\n",
    "rfc_randomized = RandomizedSearchCV(rfc, param_dict, cv = kfold, n_iter = 20, verbose = 2, n_jobs = -1, random_state = 42).fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "print(\"Best Parameter: {}\".format(rfc_randomized.best_params_))\n",
    "print(\"Test set Score: {:.3f}\".format(rfc_randomized.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Prediction error on test data\n",
    "rfc_pred_vals = rfc_randomized.predict(X_test_scaled)\n",
    "print(rfc_pred_vals)\n",
    "\n",
    "# Kfold Cross Validation\n",
    "print(\"Best Cross-Validation Score: {:.3f}\".format(rfc_randomized.best_score_))\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(rfc_randomized.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used Random Forest Classifier with default parameter with scaled data first, the test set score for that is 0.836 and the k-fold cross validation score is 0.831.\n",
    "\n",
    "I then used RandomizedSearchCV with unscaled data (GridSearchCV takes too long in this case) to tune the 'criterion', 'max_depth', 'n_estimators', 'min_samples_leaf', 'max_features' and 'oob_score' parameter, the best 'criterion' parameter for that is 'gini', 'max_depth' is 9, 'n_estimators' is 1000, 'min_samples_leaf' is 2, 'max_features' is 'log2', and 'oob_score' is True. The test set score in this situation is 0.844 and the k-fold cross validation score is 0.846.\n",
    "\n",
    "This model did predict test data better than my previous decision tree, KNN and logistic model with standard scaled data. Random Forest's final test set score with tuned parameter is 0.844 comparing to decision tree's 0.828, Logistic regression's 0.749 and KNN's 0.820; The best cross-validation score with kfold and tuned parameter using random forest is 0.846 compared to decision tree's 0.839, logistic's 0.752 and KNN's 0.832."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Now rerun your best model from questions 8 through 11, but this time add three new variables to the model that you think will increase prediction accuracy.   Did this model predict test data better than your previous models?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm adding 3 new varaibles to the random forest model that I think will increase prediction accuracy: 'word_freq_mail:', 'char_freq_$:' and 'word_freq_order:'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: spam, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>capital_run_length_total:</th>\n",
       "      <th>char_freq_!:</th>\n",
       "      <th>word_freq_address:</th>\n",
       "      <th>word_freq_mail:</th>\n",
       "      <th>char_freq_$:</th>\n",
       "      <th>word_freq_order:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>278</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1028</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2259</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>191</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>191</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   capital_run_length_total:  char_freq_!:  word_freq_address:  \\\n",
       "0                        278         0.778                0.64   \n",
       "1                       1028         0.372                0.28   \n",
       "2                       2259         0.276                0.00   \n",
       "3                        191         0.137                0.00   \n",
       "4                        191         0.135                0.00   \n",
       "\n",
       "   word_freq_mail:  char_freq_$:  word_freq_order:  \n",
       "0             0.00         0.000              0.00  \n",
       "1             0.94         0.180              0.00  \n",
       "2             0.25         0.184              0.64  \n",
       "3             0.63         0.000              0.31  \n",
       "4             0.63         0.000              0.31  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df1['spam']\n",
    "X = df1[['capital_run_length_total:', 'char_freq_!:', 'word_freq_address:', 'word_freq_mail:', 'char_freq_$:', 'word_freq_order:']]\n",
    "\n",
    "print(y[0:5])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4601, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3450, 6)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train test split, setting random_state = 42\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "print(X.shape)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using standard scaler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.977\n",
      "Test set score: 0.868\n",
      "[0 0 0 ... 1 0 0]\n",
      "Mean Cross Validation, KFold: 0.875\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier with default parameter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RandomForestClassifier = RandomForestClassifier(random_state=42)\n",
    "rfc = RandomForestClassifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Training set score: {:.3f}\".format(rfc.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(rfc.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Prediction error on test data\n",
    "rfc_pred_vals = rfc.predict(X_test_scaled)\n",
    "print(rfc_pred_vals)\n",
    "\n",
    "# Kfold Cross Validation\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(rfc, X_train_scaled, y_train, cv = kfold))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best Parameter: {'oob_score': True, 'n_estimators': 1000, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 9, 'criterion': 'gini'}\n",
      "Test set Score: 0.870\n",
      "[0 0 0 ... 1 0 0]\n",
      "Best Cross-Validation Score: 0.877\n",
      "{'bootstrap': True,\n",
      " 'ccp_alpha': 0.0,\n",
      " 'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': 9,\n",
      " 'max_features': 'log2',\n",
      " 'max_leaf_nodes': None,\n",
      " 'max_samples': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_samples_leaf': 2,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 1000,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': True,\n",
      " 'random_state': 42,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# Using RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_dict = {\n",
    "    \"criterion\":['gini','entropy'],\n",
    "    \"max_depth\":range(1,10),\n",
    "    \"n_estimators\":[200, 500, 1000, 2000],\n",
    "    \"min_samples_leaf\":range(1,5),\n",
    "    \"max_features\": ['auto', 'sqrt', 'log2'],\n",
    "    \"oob_score\": [True, False]\n",
    "}\n",
    "\n",
    "rfc_randomized = RandomizedSearchCV(rfc, param_dict, cv = kfold, n_iter = 20, verbose = 2, n_jobs = -1, random_state = 42).fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "print(\"Best Parameter: {}\".format(rfc_randomized.best_params_))\n",
    "print(\"Test set Score: {:.3f}\".format(rfc_randomized.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Prediction error on test data\n",
    "rfc_pred_vals = rfc_randomized.predict(X_test_scaled)\n",
    "print(rfc_pred_vals)\n",
    "\n",
    "# Kfold Cross Validation\n",
    "print(\"Best Cross-Validation Score: {:.3f}\".format(rfc_randomized.best_score_))\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(rfc_randomized.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters for Random Forest using RandomizedSearchCV stayed the same as the ones in question 11.\n",
    "However, this model did predict test data even better than my previous models. The best test score went from 0.844 (Random Forest with 3 X variables) to 0.870 (Random Forest with 6 X variables). And the best cross-validation score went from 0.846 (Random Forest with 3 X variables) to 0.877 (Random Forest with 6 X variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Rerun all your other models with this final set of six variables, evaluate prediction error, and choose a final model.  Why did you select this model among all of the models that you ran?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameter: {'n_neighbors': 9}\n",
      "Test set Score: 0.843\n",
      "[0 0 0 ... 1 0 0]\n",
      "Best Cross-Validation Score: 0.849\n"
     ]
    }
   ],
   "source": [
    "knn_param_grid = {'n_neighbors': range(1, 10)}\n",
    "knn_grid = GridSearchCV(KNeighborsClassifier(), knn_param_grid, cv = kfold).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best Parameter: {}\".format(knn_grid.best_params_))\n",
    "print(\"Test set Score: {:.3f}\".format(knn_grid.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Prediction error on test data\n",
    "y_pred_vals = knn_grid.predict(X_test_scaled)\n",
    "print(y_pred_vals)\n",
    "\n",
    "# Kfold Cross Validation\n",
    "print(\"Best Cross-Validation Score: {:.3f}\".format(knn_grid.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameter: {'C': 10, 'penalty': 'l2'}\n",
      "Test set Score: 0.805\n",
      "[0 0 0 ... 1 0 0]\n",
      "Best Cross-Validation Score: 0.808\n"
     ]
    }
   ],
   "source": [
    "logreg_param_grid = {'penalty': ['none','l1','l2'],\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "logreg_grid = GridSearchCV(LogisticRegression(), logreg_param_grid, cv = kfold).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best Parameter: {}\".format(logreg_grid.best_params_))\n",
    "print(\"Test set Score: {:.3f}\".format(logreg_grid.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Prediction error on test data\n",
    "l_pred_vals = logreg_grid.predict(X_test_scaled)\n",
    "print(l_pred_vals)\n",
    "\n",
    "# Kfold Cross Validation\n",
    "print(\"Best Cross-Validation Score: {:.3f}\".format(logreg_grid.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameter: {'criterion': 'gini', 'max_depth': 6, 'min_samples_leaf': 3, 'min_samples_split': 9}\n",
      "Test set Score: 0.853\n",
      "[0 0 0 ... 1 0 0]\n",
      "Best Cross-Validation Score: 0.864\n"
     ]
    }
   ],
   "source": [
    "param_dict = {\n",
    "    \"criterion\":['gini','entropy'],\n",
    "    \"max_depth\":range(1,10),\n",
    "    \"min_samples_split\":range(1,10),\n",
    "    \"min_samples_leaf\":range(1,5)\n",
    "}\n",
    "\n",
    "tree_grid = GridSearchCV(DecisionTreeClassifier(random_state=42), param_dict, cv = kfold).fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "print(\"Best Parameter: {}\".format(tree_grid.best_params_))\n",
    "print(\"Test set Score: {:.3f}\".format(tree_grid.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Prediction error on test data\n",
    "tree_pred_vals = tree_grid.predict(X_test_scaled)\n",
    "print(tree_pred_vals)\n",
    "\n",
    "# Kfold Cross Validation\n",
    "print(\"Best Cross-Validation Score: {:.3f}\".format(tree_grid.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm choosing Random Forest Classifier for my final model. Because among all the models I ran (KNN, logisitc, decision tree, and random forest), random forest classifier has the highest test set score (0.870) with tuned parameters compared to others (KNN's 0.843, logistic's 0.805, decision tree's 0.853) and it also has the highest best cross-validation scroe (0.877) with kfold and tuned parameters compared to others (KNN's 0.849, logistic's 0.808, decision tree's 0.864). Since I want a generalized model that predict unseen data better, I'm choosing random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. What variable that currently is not in your model, if included, would be likely to increase your final model's predictive power?  For this answer try to speculate about a variable outside the variables available in the data that would improve you model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would say the total number of misspelled words in the email could be a variable that will likely to increase my final model's predictive power. \n",
    "\n",
    "Since spam emails are usually poorly written with grammar and spelling errors, just like odd use of punctuations (which this dataset did include), the total number of misspelled words could be an indicator of spams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Lastly, you have listed each of the models that we have learned to use to predict dependent variables like spam.  List each model we have focused on in class thus far that you could use to evaluate data with a continuous dependent variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Linear Regression (Ordinary Least Squares)\n",
    "2. Regularized regression Lasso \n",
    "3. Regularized regression Ridge\n",
    "4. K-nearest neighbors regressor\n",
    "5. Support vector regression\n",
    "6. Decision tree regressor\n",
    "7. Ensemble methods (Bagging regressor, Boosting regressor) \n",
    "8. Random forest regression (another ensemble method)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
